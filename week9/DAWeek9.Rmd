---
title: "Generalized Linear Models"
author: "Robert Edwards"
date: "8 March 2019"
output:
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
header-includes: 
  \usepackage{float} 
  \floatplacement{figure}{H}
---

```{r setup, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, comment = NA)
```

```{r libraries, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(moderndive)
library(skimr)
library(kableExtra)
library(gridExtra)
library(janitor)
library(infer)
library(broom)
```

#Introduction {#sec:Intro}

In Weeks 3 and 6 we looked at modelling data using linear regression models where we had:

+ a **continuous response variable $y$** and
+ one or more **explanatory variables** $x_1, x_2, \dots ,x_p$ which were **numerical** and/or **categorical** variables.

Recall that for data $(y_i,x_i), i=1, \dots, n$, where $y$ is a continuous response variable, we can write a simple linear regression model as follows: 
$$y_i = \alpha + \beta x_i + \epsilon_i, ~~~ \epsilon_i \sim N(0, \sigma^2)$$

where,

+ $y_i$ is the $i^{th}$ observation of the continuous response variable;
+ $\alpha$ is the **intercept** of the regression line;
+ $\beta$ is the **slope** of the regression line;
+ $x_i$ is the $i^{th}$ observations of the explanatory variable;
+ $\epsilon_i$ is the $i^{th}$ random component.

Thus, the full probability model for $y_i$ given $x_i(y_i|x_i) can be written as
$$y_i|x_i \sim N(\alpha + \beta x_i, \sigma^2)$$

where the mean $\alpha + \beta x_i$ is given by the deterministic part of the model and the variance $\sigma^2$ by the random part.  Hence we make the assumption that the outcomes $y_i$ are normally distributed with mean $\alpha + \beta x_i$ and variance $\sigma^2$.  However, what if our response variable $y$ is not a continuous random variable?

##Generalized Linear Models
The main objective this week is to introduce **Generalised Linear Models (GLMs)**, which extend the linear model framework to response variables that don't follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. Here we shall focus on binary/categorical response variables. The generalised linear model can be written as:  
$$
\begin{aligned}
y_i             &\sim f(g(\mathbf{\mu}_i)) \\
\mathbf{\mu}_i  &= \mathbf{x}^T_i \mathbf{\beta}
\end{aligned}
$$

where the response $y_i$ is predicted through the linear combination $\mathbf{\mu}_i$ of explanatory variables by the link function $g(\cdot)$, assuming some distribution $f(\cdot)$ for $y_i$, and $\mathbf{x}^T_i$ is the $i^{th}$ row of the design matrix $\mathbf{X}$.  For example, the simple linear regression model above for a continuous response variable has the normal distribution as $f(\cdot)$, with corresponding link function equal to the identity function, that is, $g(\mathbf{\mu}_i) = \mathbf{\mu}_i$.  

What if our response variable $y$ is binary (e.g. yes/no, success/failure, alive/dead)?  That is, the independent responses $y_i$ can either be:  

+ **binary**, taking the value 1 (say success, with probability $p_i$) or 0 (failure, with probability $1-p_i$) or
+ **binomial**, where $y_i$ is the number of successes in a given number of trial $n_i$, with the probability of success being $p_i$ and the probability of failure being $1-p_i$  

In both cases the distribution of $y_i$ is assumed to be binomial, but in the first case it is $\mbox{Bi}(1, p_i)$ and in the second case it is $\mbox{Bi}(n_i, p_i)$.  Hence, a binary response variable $y_i$ has a binomial distribution with corresponding link function $g(\cdot)$ equal to the **logit link** function, that is  
$$g(p_i) = \mbox{log} \Bigg( \frac{p_i}{1-p_i} \Bigg)$$

which is also referred to as the **log-odds** (since $\frac{p_i}{1-p_i}$ is an odds ratio).  Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success $p_i$, and as we know probabilities must be between 0 and 1 $(p_i \in [0,1])$. So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that
$$p_i = \mathbf{x}^T_i \mathbf{\beta}$$  
we would need to ensure that in some way $\mathbf{x}^T_i \mathbf{\beta} \in [0,1]$ that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that
$$\mbox{log} \Bigg( \frac{p_i}{1-p_i} \Bigg) = \mathbf{x}^T_i \mathbf{\beta}$$

No restrictions need to be in place on our estimates of the parameter vector $\beta$, since the inverse of the logit link function will always give us valid probabilities since 
$$p_i = \frac{\mbox{exp}(\mathbf{x}^T_i \mathbf{\beta})}{1 + \mbox{exp}(\mathbf{x}^T_i \mathbf{\beta})}  ~~~~ \in [0,1]$$

This linear regression model with a binary response variable is referred to as **logistic regression**. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.

Model	    Random Component	                              Systematic Component	                                      Link Function	   
--------- ----------------------------------------------  ---------------------------------------------------------   ---------------------------------  
Normal	  $y_i \stackrel{indep}{\sim} N(\mu_i,\sigma^2)$	$\mathbf{x}^T_i \mathbf{\beta}=\beta_0+\beta_1x_i+\dots$    $g(\mu_i)=\mu_i$             
Logistic	$y_i \stackrel{indep}{\sim} \mbox{Bi}(1,p_i)$	  $\mathbf{x}^T_i \mathbf{\beta}=\beta_0+\beta_1x_i+\dots$    $g(\mu_i)=\mbox{log}\bigg(\frac{\mu_i}{1-\mu_i}\bigg)=\mbox{log}\bigg(\frac{p_i}{1-p_i}\bigg)$     
--------- ----------------------------------------------  ---------------------------------------------------------   ---------------------------------  


\newpage
#Binary Logistic Regression with One Numerical Explanatory Variable
Here we shall begin by fitting a logistic regression model with one numerical explanatory variable. Let's return to the `evals` data from the `moderndive` package that we examined in Week 3.

##Teaching Evaluation Scores
Recall from previous weeks that student feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see Economics of Education Review for details. Here, we shall return to the study of student evaluations of $n=463$ professors from The University of Texas at Austin.

Previously, we looked at **teaching score** as our continuous response variable and **beauty score** as our explanatory variable. Now we shall consider **gender** as our response variable, and hence shall have a binary response variable (female/male). We will examine if there is any difference in **gender** by **age** of the teaching instructors within the `evals` data set.

First, let's start by selecting the variables of interest from the `evals` data set:

```{r evals, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
evals.gender <- evals %>%
  select(gender, age)
```

Now, let's look at a boxplot of `age` by `gender` to get an initial impression of the data:
```{r boxplot1, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '50%', fig.cap = "\\label{fig.plot1}Teaching instructor age by gender"}
ggplot(data = evals.gender, aes(x = gender, y = age, fill = gender)) +
  geom_boxplot() +
  labs(x = "Gender", y = "Age") +
  theme(legend.position = "none")
```

Here we can see that the male teaching instructors tend to be older than that of their female colleagues. Now, let's fit a logistic regression model to see whether age is a significant predictor of the odds of a teaching instructor being male or female.

##Log-odds
To fit a logistic regression model we will use the generalised linear model function `glm`, which acts in a very similar manner to the `lm` function we have used previously. We only have to deal with an additional argument. The logistic regression model with **gender** as the response and **age** as the explanatory variable is given by:

```{r model1, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
model <- glm(gender ~ age, data = evals.gender, family = binomial(link = "logit"))
```

Here we include the additional `family` argument, which states the distribution and link function we would like to use. Hence `family = binomial(link = "logit")` states we have a binary response variable, and thus have a binomial distribution, with its corresponding **logit link** function. Now, let's take a look at the summary produced from our logistic regression model:

```{r echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
model %>%
  summary()
```

Firstly, the baseline category for our binary response is `female`.  This is due to the default baseline in R being taken as the one which comes first alphabetically, which can be seen from the `levels` function:

```{r echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
levels(evals.gender$gender)
```

This means that estimates from the logistic regression model are for a change on the **log-odds** scale for `males` in comparison to the response baseline `females`.  That is
$$\mbox{ln} \bigg( \frac{p}{1-p} \bigg) = \alpha + \beta \cdot \mbox{age} = `r round(model$coefficients[1],2)` + `r round(model$coefficients[2],2)` \cdot \mbox{age}$$

where $p = \mbox{Prob(Male)}$ and $1-p = \mbox{Prob(Female)}$.  Hence, the **log-odds** of the instructor being male increase by `r round(model$coefficients[2],2)` for every one unit increase in `age`.  This provies us with a point estimate of how the log-odds changes with age, however, we are also interested in producing a 95% confidence interval for these log-odds.  This can be done using the `confit` function in the `MASS` package:

```{r echo = TRUE, eval = FALSE, warning = FALSE, message = FALSE}
confint(model) %>%
  kable(booktabs = TRUE) %>%
  kable_styling(latex_options = "hold_position",
                format = "latex")
```

```{r echo = TRUE, eval = FALSE, warning = FALSE, message = FALSE}
confint(model)
```

To understand how these endpoints are calculated, consider the following code:

```{r echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
mod.coef.logodds <- model %>%
  summary() %>%
  coef()
```

```{r echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
age.logodds.lower <- mod.coef.logodds["age", "Estimate"] - 1.96 * mod.coef.logodds["age", "Std. Error"]
age.logodds.upper <- mod.coef.logodds["age", "Estimate"] + 1.96 * mod.coef.logodds["age", "Std. Error"]
```

Hence the point estimate for the log-odds is `r round(model$coefficients[2],2)`, which has a corresponding 95% confidence interval of (`r age.logodds.lower`, `r age.logodds.upper`).  This can be displayed graphically using the `plot_model` function from the `sjPlot` package simply passing our `model` as an argument:
```{r plot2, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, out.width = '50%', fig.cap = "\\label{fig.plot2}The log-Odds of age for Male instructors"}
plot_model(model, show.values = TRUE, transform = NULL, title = "Log-Odds (Male instructor)", show.p = FALSE)
```

Some of the interesting arguments that can be passed to the `plot_model` function are:

+ `show.values` = TRUE/FALSE`: Whether the log-odds/odds values should be displayed;
+ `show.p = TRUE/FALSE`: Adds asterisks that indicate the significance level of estimates to the value labels;
+ `transform`: A character vector naming the function that will be applied to the estimates. The default transformation uses exp to display the odds ratios, while transform = NULL displays the log-odds; and
+ `vline.color`: colour of the vertical "zero effect" line.

Further details on using `plot_model` can be found here and here.

Now, let's add the estimates of the log-odds to our data set:







```{r echo = TRUE, eval = FALSE, warning = FALSE, message = FALSE}
evals.gender <- evals.gender %>%
                  mutate(probs.male = fitted(model))

ggplot(data = evals.gender, aes(x = age, y = probs.male)) +
  geom_dotplot(dotsize = 0.6, alpha = 0.2, aes(fill = gender), binwidth = 1, stackgroups = TRUE) +
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"),
              se = FALSE) +
  labs(x = "Age", y = "Probability of instructor being male")
```




















